# absa-for-product-reviews

## Download Spacy model
```
python3 -m spacy download en_core_web_md
```

## Source data
* [SemEval-2014 Task 4](https://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools)
* [SemEval-2014 ABSA Test Data - Gold Annotations](http://metashare.elda.org/repository/browse/semeval-2014-absa-test-data-gold-annotations/b98d11cec18211e38229842b2b6a04d77591d40acd7542b7af823a54fb03a155/)
* [SemEval-2016 Task 5](https://alt.qcri.org/semeval2016/task5/index.php?id=data-and-tools)

## References
* [SemEval-2014 Task 4: Aspect Based Sentiment Analysis](https://aclanthology.org/S14-2004.pdf)
* [SemEval-2016 Task 5: Aspect Based Sentiment Analysis](https://aclanthology.org/S16-1002.pdf)
* [Aspect Based Sentiment Analysis](https://medium.com/analytics-vidhya/aspect-based-sentiment-analysis-5a78d4cba1b1)
* [Natural Language Processing with Transformers: Building Language Applications with Hugging Face](https://www.amazon.com/Natural-Language-Processing-Transformers-Applications/dp/1098103246)
* [Splitting a Dataset for Machine Learning](https://madewithml.com/courses/mlops/splitting/)
* [A detailed example of how to generate your data in parallel with PyTorch](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel)
* [RANZCR: Multi-Head Model [training]](https://www.kaggle.com/code/ttahara/ranzcr-multi-head-model-training)
* [Tutorial 6: Transformers and Multi-Head Attention](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)


## Use cleantext to clean the reviews
https://pypi.org/project/cleantext/
